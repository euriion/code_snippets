
 
# Comsine Similarity - 코사인 유사도
 
위키피디아를 좋아하신다면 당연히 위키피디아를 참조하시면 되겠습니다. 

[영어 위키피디아 - Cosine Similarity](http://en.wikipedia.org/wiki/Cosine_similarity)
[한국어 위키피디아 - Cosine Similarity](http://ko.wikipedia.org/wiki/%EC%BD%94%EC%82%AC%EC%9D%B8_%EC%9C%A0%EC%82%AC%EB%8F%84)

한국어 위키피디아 보다는 영문 위키피디아가 정리가 잘 되어 있습니다.
(하지만 위키피디아 울렁증이 있다면...)
 
### 기본 설명 (대체 뭔가?)

쉽게 설명하면 두 벡터(Vector)의 유사도(Similarity)를 구하는 방법입니다.
유사도를 구할 때 두 벡터 사이의 각을 코사인(Cosine)값을 구해서 유사도로 취급하기 때문에 코사인 유사도(Cosine Similarity)라고 부릅니다. 그게 전부입니다.  (우얼...@.@)

### 용도 (어따 쓰나?)

아마도 궁금할 것이 "두 벡터의 유사도를 구해서 어따 쓰는가?" 라는 것일텐데요.

> 벡터는 무엇이고 유사도는 무엇인가? 'ㅡ'

1. 검색 엔진에서 검색어(Query)와 문서(Document)의 유사도를 구해서 가장 유사도가 높은 것을 먼저 보여주기 위한 기초 랭킹 알고리즘으로 사용됩니다.  
> 그렇다고 검색 랭킹을 이것으로 다 하는 것은 아닙니다. 그냥 기본입니다.
정보추출 책이나 자료를 찾아보면 벡터 스페이스 모델 (Vector Space Model)에서 문서(Document)간의 유사도를 구하기 위해서 쓴다고 되어 있을 것입니다.  그래서 Consine Similarity라고 하면 문서의 유사도를 구한다고 대부분 생각하지만. 꼭 거기에만 쓰이는 것은 아닙니다.  항상 TF-IDF(Term Frequency - Inverse Document Frequency)에 대한 내용이 같이 언급되는 이유이기도 합니다.
2. 그 외에도 다른 분석에서도 유사도를 구할 때 사용합니다.
> 가끔 나옵니다만 흔치는 않습니다.  이걸 사용해서 할 만한 것이 예상 외로 적습니다. 
3. 클러스터링(Clustering), 그러니까 군집화 모델에서도 쓰입니다.
> 벡터의 유사도를 구해야 비슷한 것끼리 묶지요. 그러니까 여기다도 씁니다. 클러스터링을 모르시면 그냥 패쓰! 

벡터 2개의 유사도를 구한다고 했지만 벡터 2개만 가지고는 유사도값이 하나만 나오기 때문에 아무짝에도 쓸모가 없습니다.   
유사도를 구하는 목적의 근본 문제가 $A$가 $B$와 유사한지 $A$와 $C$가 더 유사한지 등을 알고 싶어하는 경우가 대부분이기 때문입니다.
그래서 여러 개의 벡터를 대상으로  각각 유사도를 구해서 가장 유사도가 높은 순으로 정렬해서 사용하는 경우가 많습니다.  
> 클러스터링을 할 때도 마찬가지겠지요

벡터의 개수 즉, 비교할 데이터가 $n$개고 벡터로 표현할 수 있다면 $\frac{n\times(n-1)}{2}$번 만큼 연산을 해야 합니다.

> RDMBS에 100개의 레코드가 있고 컬럼이 여러개 있는데 모두 숫자라면 $\frac{100(100-1)}{2}$ 만큼입니다. 에... 4950번 입니다

### 조건

- 두 벡터의 원소들은 모두 양수(플러스!)여야 합니다. x, y 직교 좌표축에서 1사분면에 오는 것들입니다.  (모눈종이에서 오른쪽 위)
> 그래서 원소들이 음수가 되지 않는 문제에만 갖다 씁니다.
- 벡터의 원소수는 같아야 합니다.
> 너무 당연한 것입니다. 비교하는 벡터의 원소 갯수가 일치하지 않으면 각각 빠진 것을 0으로 채워서 동일하게 만들어야 합니다.  벡터의 원소 갯수가 좌표축에서의 축의 갯수이기 때문입니다. (ㅇㅇ?)

### 특징

1사분면의 두 벡터의 코사인 값은 0 ~ 1 사이의 값입니다.  벡터의 각이 작을 수록 1에 가까워지고 클수록 0에 가까워집니다.   따라서 결과를 재가공하지 않고 바로 쓰기 편합니다.
두 벡터가 정확히 직교이면 0이 됩니다.

### 공식

$\text{similarity} = \cos(\theta) = {A \cdot B \over \|A\| \|B\|} = \frac{ \sum\limits_{i=1}^{n}{A_i \times B_i} }{ \sqrt{\sum\limits_{i=1}^{n}{(A_i)^2}} \times \sqrt{\sum\limits_{i=1}^{n}{(B_i)^2}} }$

아주 간단한 공식입니다.
(만은...  고등학교때 배웠던것 같지만.. 기억은 안나고..)

분자 부분과 분모 부분을 나눠서 설명하면 다음과 같습니다.

#### 분자 (벡터 내적)

$\ll A \cdot B \gg$는 벡터의 내적(dot product)입니다. 
두 벡터의 각 원소들을 순서대로 짝맞춰서 곱한 다음에 결과들을 다 더하면 됩니다.
내적은 잘 기억이 안나시면 밑에 예제를...  

아래와 같은 두 벡터가 있다면
$A = (1,2,3,4,5)$  
$B = (6,7,8,9,10)$


* 각각 곱합니다. 순서를 맞춰서 잘~
$1 \times 6 = 6$
$2 \times 7 = 14$
$3 \times 8 = 24$
$4 \times 9 = 36$
$5 \times 10 = 50$

* 곱한 것을 다 더합니다
$ 6 + 14 + 24 + 36 + 50 = 130 $

#### 분모 (두 벡터의 크기를 곱한다)

$\|A\|$ 는 A벡터의 크기를 말합니다.
$\|B\|$ 는 B벡터의 크기를 말합니다.

분모는 두벡터의 크기를 구해서 곱하면 되는데요 벡터의 크기는 사실 '피타고라스'입니다. 
> 삼각형의 빗변의 길이 구하기를 기억하신다면...
$C=\sqrt{ A^2 + B^2 }$

* A벡터의 크기를 구합니다

$\sqrt{ 1^2 + 2^2 + 3^2 + 4^2 + 5^2 } = 7.4161984870957$

* B벡터의 크기를 구합니다

$\sqrt{6^2 + 7^2 + 8^2 + 9^2 + 10^2} = 18.1659021245849$

* 곱합니다

$7.4161984870957 \times 18.1659021245849 = 134.7219358530751$

#### 마무리 계산

분자를 분모로 나눕니다.

$\frac{130}{134.7219358530751}=0.9649505$

위 값이 코사인 유사도 값입니다.

-------

텍스트 데이터를 가지고 Cosine Simliary를 구해서 문서간의 유사도를 구하는 것은 다음 포스트에 적을예정입니다.

 